{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning (2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory of Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers practical machine learning knowledge and deep neural networks, including feedforward and convolutional neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory and Knowledge\n",
    "<span style=\"color:red\">Activation function plays an important role in modern Deep NNs. For Sigmoid and tanh, the range  and derivative are shown below. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Sigmoid</u>: \n",
    "$$\\sigma(x) = \\dfrac{1}{1+e^{-x}}$$\n",
    "\n",
    "The input range for $\\sigma(x)$ is $-\\infty\\leq x\\leq \\infty$, so we can check three cases to find the output range of $\\sigma(x)$:\n",
    "\n",
    "Case 1. $x\\rightarrow-\\infty$:\n",
    "\n",
    "As $x$ approaches $-\\infty$, the $e^{-x}\\rightarrow\\infty$, which implies $\\sigma(x)\\rightarrow 0$.\n",
    "\n",
    "Case 2. $x = 0$:\n",
    "\n",
    "As $x=0$, the $e^{0}=1$, which implies $\\sigma(0)=\\dfrac{1}{2}$.\n",
    "\n",
    "Case 3. $x\\rightarrow\\infty$:\n",
    "\n",
    "As $x$ approaches $\\infty$, the $e^{-x}\\rightarrow 0$, which implies $\\sigma(x)\\rightarrow 1$.\n",
    "\n",
    "Therefore, we see $\\sigma(x)\\in(0,1)$\n",
    "\n",
    "The derivative for $\\sigma(x)$ is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\dfrac{d\\sigma}{dx} & = \\dfrac{d}{dx}(1+e^{-x})^{-1}\\\\\n",
    "& = e^{-x}(1+e^{-x})^{-2}\\\\\n",
    "& = \\dfrac{e^{-x}}{(1+e^{-x})^2}\\\\\n",
    "& = \\sigma(x)\\dfrac{e^{-x}}{1+e^{-x}}\\\\\n",
    "& = \\sigma(x)\\left(\\dfrac{1+e^{-x}}{1+e^{-x}}-\\dfrac{1}{1+e^{-x}}\\right)\\\\\n",
    "& = \\sigma(x)\\left(1-\\sigma(x)\\right)\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where the chain rule was used $\\dfrac{d}{dx}(u(v(x))=\\dfrac{d}{dv}(u)\\dfrac{d}{dx}(v)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Tanh</u>: \n",
    "$$\\sigma(x) = \\dfrac{e^{x} - e^{-x}}{e^{x} + e^{-x}}= \\dfrac{1-e^{-2x}}{1+e^{-2x}}$$\n",
    "\n",
    "Case 1. $x\\rightarrow-\\infty$:\n",
    "\n",
    "As $x$ approaches $-\\infty$, the $e^{-x}\\rightarrow\\infty$, which implies $\\sigma(x)\\rightarrow -1$.\n",
    "\n",
    "Case 2. $x = 0$:\n",
    "\n",
    "As $x=0$, the $e^{0}=1$, which implies $\\sigma(0)=0$.\n",
    "\n",
    "Case 3. $x\\rightarrow\\infty$:\n",
    "\n",
    "As $x$ approaches $\\infty$, the $e^{-x}\\rightarrow 0$, which implies $\\sigma(x)\\rightarrow 1$.\n",
    "\n",
    "Therefore, we see $\\sigma(x)\\in(-1,1)$\n",
    "\n",
    "The derivative for $\\sigma(x)$ is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\dfrac{d\\sigma}{dx} & = \\dfrac{d}{dx}\\left(\\dfrac{e^x-e^{-x}}{e^x+e^{-x}}\\right)\\\\\n",
    "& = \\dfrac{(e^x+e^{-x})(e^x+e^{-x})- (e^x-e^{-x})(e^x-e^{-x})}{(e^x+e^{-x})^2}\\\\\n",
    "& = 1 - \\dfrac{(e^x-e^{-x})^2}{(e^x+e^{-x})^2}\\\\\n",
    "& = 1 - \\sigma(x)^2\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where the quotient rule was used $\\dfrac{d}{dx}\\left(\\dfrac{u}{v}\\right)= \\dfrac{\\tfrac{d}{dx}(u)v-u\\tfrac{d}{dx}(v)}{v^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Softmax activation aims to transform discriminative values to prediction probabilities.</span>\n",
    "\n",
    " Consider a classification task with $M=4$ classes and a data example $x$ with a ground-truth label $y=2$. Assume that at the output layer of a feed-forward neural network, we obtain the logits $h^{L}=[2,-1,5,0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{softmax}(h_m)= \\dfrac{e^{h_m}}{\\sum_{i=1}^Me^{h_i}},\\, \\text{ for } m=1,\\cdots,M\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, we can calculate the corresponding probabilities:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p(x) & = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{e^2}{e^2+e^{-1}+e^5+e^0}\\\\\n",
    "\\dfrac{e^{-1}}{e^2+e^{-1}+e^5+e^0}\\\\\n",
    "\\dfrac{e^5}{e^2+e^{-1}+e^5+e^0}\\\\\n",
    "\\dfrac{e^0}{e^2+e^{-1}+e^5+e^0}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "0.0470\\\\\n",
    "0.0023\\\\\n",
    "0.9443\\\\\n",
    "0.0064\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "4.70\\%\\\\\n",
    "0.23\\%\\\\\n",
    "94.43\\%\\\\\n",
    "0.64\\%\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "We can calculate the cross-entropy loss caused by the feed-forward neural network at $(x,y)$.\n",
    "\n",
    "The general formula for cross entropy is given by\n",
    "\n",
    "\\begin{equation}\n",
    "CE = - \\sum_i y_i\\log(p_i)\n",
    "\\end{equation}\n",
    "where $y_i$ is the true label and $p_i$ is the probability.\n",
    "\n",
    "Given the ground truth is $y=2$, we can say for $y_2=1$ and $y_1=y_3=y_4=0$. Therefore, the cross entropy for this prediction is \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "CE & = -\\log(0.0023)\\\\\n",
    "& = 6.057\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question a)\n",
      "\t[0.04701312 0.00234065 0.9442837  0.00636253]\n",
      "\n",
      "Question b)\n",
      "\t6.0573286242556375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "from math import log\n",
    "\n",
    "def softmax(data):\n",
    "    result = np.zeros(len(data))\n",
    "    total = 0\n",
    "    for row in range(len(data)):\n",
    "        result[row] = exp(float(data[row]))\n",
    "        total += result[row]\n",
    "    return result/total\n",
    "\n",
    "def CE_loss(y, p):\n",
    "    result = 0\n",
    "    for row in range(len(y)):\n",
    "        result -= y[row]*log(p[row])\n",
    "    return result\n",
    "    \n",
    "truth = np.array([0,1,0,0])\n",
    "h = np.array([2,-1,5,0])\n",
    "p = softmax(h)\n",
    "loss = CE_loss(truth,p)\n",
    "print(\"Question a)\\n\\t{}\".format(p))\n",
    "print(\"\\nQuestion b)\\n\\t{}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Linear operation and element-wise activation are two building-blocks for conducting a layer in a feedforward neural network.</span>\n",
    "\n",
    "\n",
    "Assuming that hidden layer $1$ has value \n",
    "$$h^1(x)= \\left[\\begin{array}{ccc}\n",
    "1.5 & 2.0 \\end{array}\\right]^T$$\n",
    "\n",
    "and the weight matrix and bias at the second layer are:\n",
    "$$W^{2}=\\left[\\begin{array}{cc}\n",
    "-1 & -1\\\\\n",
    "-1 & 1\\\\\n",
    "-1 & 0\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "We can calcualte the value of the hidden layer $\\bar{h}^{2}(x)$ after applying *the linear operation* with the matrix $W^2$ and the bias $b^2$ over $h^1$.\n",
    "\n",
    "\n",
    "The value at $\\bar{h}^{2}(x)$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{h}^{2}(x) = W^2h^1+b^2\n",
    "\\end{equation}\n",
    "and given there is no bias for hidden layer 2, we can calculate the value to be\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\bar{h}^{2}(x) & =\n",
    "\\begin{bmatrix}\n",
    "-1 & -1\\\\\n",
    "-1 & 1\\\\\n",
    "-1 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1.5 \\\\\n",
    "2.0 \n",
    "\\end{bmatrix}\\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "-3.5 \\\\\n",
    "0.5 \\\\\n",
    "-1.5\n",
    "\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that we apply *the ReLU activation function* at the second layer. What is the value of the hidden layer $h^2(x)$ after we apply the activation function?\n",
    "\n",
    "The ReLU function is\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{ReLU}(z) = \\max(0,z)\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, the values to the hidden layer after the activation is applied is\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "h^2(x) = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0.5 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question a)\n",
      "[[-3.5]\n",
      " [ 0.5]\n",
      " [-1.5]]\n",
      "\n",
      "Question b)\n",
      "[[0. ]\n",
      " [0.5]\n",
      " [0. ]]\n"
     ]
    }
   ],
   "source": [
    "h1 = np.array([[1.5],\n",
    "               [2]])\n",
    "W2 = np.array([[-1,-1],\n",
    "               [-1,1],\n",
    "               [-1,0]])\n",
    "\n",
    "h2bar = np.matmul(W2,h1)\n",
    "h2 = np.maximum(h2bar,np.zeros(h2bar.shape))\n",
    "print(\"Question a)\\n{}\".format(h2bar))\n",
    "print(\"\\nQuestion b)\\n{}\".format(h2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Multilayered feedforward neural network for a regression</span> \n",
    "\n",
    "Consider that have a network for a regression problem to predict to real-valued $y_1, y_2$, and $y_3$.\n",
    "\n",
    "The architecture of this network ($3 (Input)\\rightarrow4(ReLU)\\rightarrow 3(Output)$) is shown in the following figure:\n",
    "\n",
    "\n",
    "<img src=\"FeedforwardNN.png\" \n",
    "    style=\"display: block; \n",
    "           margin-left: auto;\n",
    "           margin-right: auto;\n",
    "           width: 65%;\n",
    "           background: white;\"\n",
    "           />\n",
    "\n",
    "We now feed a feature vector $x=\\left[\\begin{array}{ccc}\n",
    "1.2 & -1 & 2\\end{array}\\right]^{T}$ with ground-truth label $y=\\left[\\begin{array}{ccc} 1.5 & -1 & 2\\end{array}\\right]^{T}$ to the above network. \n",
    "\n",
    "**Forward propagation**\n",
    "\n",
    "What is the value of $\\bar{h}^{1}(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\bar{h}_1^1 & = W^1x+b^1\\\\\n",
    "& = \\begin{bmatrix} 1 & -1 &0\\\\0 & -1 & 1\\\\ 1 & 1 & -1\\\\1 & 1 & 1\\end{bmatrix}\n",
    "\\begin{bmatrix} 1.2\\\\ -1\\\\ 2\\end{bmatrix}\n",
    "+\\begin{bmatrix} 0\\\\ 1\\\\ 1\\\\ -1\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix} 2.2\\\\ 3\\\\ -1.8\\\\ 2.2\\end{bmatrix}\n",
    "+\\begin{bmatrix} 0\\\\ 1\\\\ 1\\\\ -1\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix} 2.2\\\\ 4\\\\ -0.8\\\\ 1.2\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the value of $h^{1}(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLU function is\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{ReLU}(z) = \\max(0,z)\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, the values to the hidden layer after the activation is applied is\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "h^1(x) & = \\text{ReLU}\\left(\\bar{h}^1\\right)\\\\\n",
    "& = \\begin{bmatrix}\n",
    "2.2 \\\\\n",
    "4 \\\\\n",
    "0\\\\\n",
    "1.2\n",
    "\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the predicted value $\\hat{y}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{y} & = W^2h^1+b^2\\\\\n",
    "& = \\begin{bmatrix} 1.5 & 1 &1 & -1\\\\0 & 0 & 1 & 1\\\\ -1 & 1 & 1 & -1\\end{bmatrix}\n",
    "\\begin{bmatrix} 2.2\\\\ 4\\\\ 0\\\\ 1.2\\end{bmatrix}\n",
    "+\\begin{bmatrix} 1\\\\ 0\\\\ 0.5\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix} 6.1\\\\ 1.2\\\\ 0.6\\end{bmatrix}\n",
    "+\\begin{bmatrix} 1\\\\ 0\\\\ 0.5\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix} 7.1\\\\ 1.2\\\\ 1.1\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the value of the L2 loss $l$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\ell_2 & = \\sqrt{\\sum_{i=1}^3\\left(y_i-\\hat{y}_i\\right)^2}\\\\\n",
    "& = \\sqrt{\\left(1.5-7.1\\right)^2+\\left(-1-1.2\\right)^2+\\left(2-1.1\\right)^2}\\\\\n",
    "& = 6.08\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward propagation**\n",
    "\n",
    "For backward propagation, we need to calculate the $\\dfrac{\\partial l}{\\partial h^{2}},\\dfrac{\\partial l}{\\partial W^{2}}$, and $\\dfrac{\\partial l}{\\partial b^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $h^2 =\\hat{y} = W^2h^1+b^2$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\dfrac{\\partial \\ell_2}{\\partial h^2} & =\\dfrac{\\partial \\ell_2}{\\partial \\hat{y}}\\\\\n",
    "& =\\begin{bmatrix} \\dfrac{y_1-\\hat{y}_1}{\\ell_2}& \\dfrac{y_2-\\hat{y}_2}{\\ell_2} & \\dfrac{y_3-\\hat{y}_3}{\\ell_2} \\end{bmatrix}\\\\\n",
    "& = \\dfrac{1}{6.08}\\begin{bmatrix}-5.6 & -2.2 & 0.9 \\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix}-0.92 & -0.36 &  0.15\\end{bmatrix} \n",
    "\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $h^2 = W^2 h^1 + b^2$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\dfrac{\\partial \\ell_2}{\\partial W^2} & = \\dfrac{\\partial \\ell_2}{\\partial h^2}\\dfrac{\\partial h^2}{\\partial W^2} \\\\\n",
    "& = \\begin{bmatrix}-0.92 \\\\ -0.36 \\\\  0.15\\end{bmatrix}  \\begin{bmatrix} 2.2 &4 &0 &1.2\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix} -2.03& -3.68& 0.    &     -1.11\\\\-0.80& -1.45&  0.     &    -0.43\\\\ 0.33 & 0.59&  0.  &        0.18\\end{bmatrix}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\dfrac{\\partial \\ell_2}{\\partial b^2} & = \\dfrac{\\partial \\ell_2}{\\partial h^2}\\dfrac{\\partial h^2}{\\partial b^2}\\\\\n",
    "& = \\begin{bmatrix}-0.92 \\\\ -0.36 \\\\  0.15\\end{bmatrix} \n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the derivatives $\\dfrac{\\partial l}{\\partial h^{1}}, \\dfrac{\\partial l}{\\partial \\bar{h}^{1}},\\dfrac{\\partial l}{\\partial W^{1}}$, and $\\dfrac{\\partial l}{\\partial b^{1}}$? \n",
    "\n",
    "Using $h^2 = W^2h^1+b^2$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\dfrac{\\partial \\ell_2}{\\partial h^1} & =\\dfrac{\\partial \\ell_2}{\\partial h^2}\\dfrac{\\partial h^2}{\\partial h^1}\\\\\n",
    "& = \\begin{bmatrix}-0.92 & -0.36 &  0.15\\end{bmatrix}   \\begin{bmatrix} 1.5 & 1 &1 & -1\\\\0 & 0 & 1 & 1\\\\ -1 & 1 & 1 & -1\\end{bmatrix}\\\\\n",
    "&  = \\begin{bmatrix}-1.53 &-0.77 & -1.13 & 0.41\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using  \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "h^1 & = \\text{ReLU}\\left(\\bar{h}^1\\right)\\\\\n",
    "\\dfrac{\\partial h^1}{\\partial \\bar{h}^1} & = \\begin{bmatrix}\\text{ReLU}^\\prime\\left(\\bar{h}^1\\right) & 0 &0\\\\0 &\\text{ReLU}^\\prime\\left(\\bar{h}^1\\right) & 0\\\\ 0 & 0 & \\text{ReLU}^\\prime\\left(\\bar{h}^1\\right)\\end{bmatrix}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{ReLU}^\\prime(x) =\\left\\{\n",
    "\\begin{matrix}\n",
    "0 &\\text{if }x \\leq 0\\\\\n",
    "1 &\\text{if }x > 0\\\\\n",
    "\\end{matrix}\\right.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\dfrac{\\partial \\ell_2}{\\partial \\bar{h}^1} & = \\dfrac{\\partial \\ell_2}{\\partial h^1}\\dfrac{\\partial h^1}{\\partial \\bar{h}^1}\\\\\n",
    "& = \\begin{bmatrix}-1.53 &-0.77 & -1.13 & 0.41\\end{bmatrix}  \\begin{bmatrix}\\text{ReLU}^\\prime\\left(2.2\\right) & 0& 0 &0\\\\0 &\\text{ReLU}^\\prime\\left(4\\right) & 0& 0\\\\ 0 & 0 & \\text{ReLU}^\\prime\\left(-0.8\\right)& 0\\\\0 & 0 & 0 & \\text{ReLU}^\\prime\\left(1.2\\right)\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix}-1.53 & -0.77 & 0 & 0.41 \\end{bmatrix}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\bar{h}^1= W^1x +b^1$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\dfrac{\\partial \\ell_2}{\\partial W^1} & =\\dfrac{\\partial \\ell_2}{\\partial \\bar{h}^1}\\dfrac{\\partial \\bar{h}^1}{\\partial W^1}\\\\\n",
    "& =\\begin{bmatrix}-1.53 \\\\ -0.77 \\\\ 0 \\\\ 0.41 \\end{bmatrix} \\begin{bmatrix} 1.2 & -1 & 2\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix}-1.83 &  1.53 & -3.06\\\\-0.93 &  0.77 & -1.55\\\\0 &  0 & 0\\\\ 0.49 & -0.41 & 0.82\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\dfrac{\\partial \\ell_2}{\\partial b^1} & =\\dfrac{\\partial \\ell_2}{\\partial \\bar{h}^1}\\dfrac{\\partial \\bar{h}^1}{\\partial b^1}\\\\\n",
    "& =\\begin{bmatrix}-1.53 \\\\ -0.77 \\\\ 0 \\\\ 0.41 \\end{bmatrix} \n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD update**\n",
    "\n",
    "Assume that we use SGD with learning rate $\\eta=0.01$ to update the model parameters. What are the values of $W^2, b^2$ and $W^1, b^1$ after updating?\n",
    "\n",
    "Significant figures given to 3 decimanl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "W_2^2 & = W_1^2 - \\eta \\dfrac{\\partial \\ell_2}{\\partial W_1^2}\\\\\n",
    "& = \\begin{bmatrix} 1.5 & 1 &1 & -1\\\\0 & 0 & 1 & 1\\\\ -1 & 1 & 1 & -1\\end{bmatrix} - 0.01\\begin{bmatrix} -2.03& -3.68& 0    & -1.11\\\\-0.80& -1.45&  0     &    -0.43\\\\ 0.33 & 0.59&  0  &  0.18\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix} 1.520 & 1.037 &  1.000 & -0.989\\\\0.008 & 0.014 &  1.000 & 1.004\\\\-1.003 &  0.9946 & 1.000 & -1.002\\end{bmatrix}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "b_2^2 & = b_1^2 - \\eta \\dfrac{\\partial \\ell_2}{\\partial b_1^2}\\\\\n",
    "& = \\begin{bmatrix} 1\\\\ 0\\\\ 0.5\\end{bmatrix} - 0.01\\begin{bmatrix}-0.92 \\\\ -0.36 \\\\  0.15\\end{bmatrix} \\\\\n",
    "& = \\begin{bmatrix} 1.009 \\\\0.004 \\\\0.499 \\end{bmatrix}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "W_2^1 & = W_1^1 - \\eta \\dfrac{\\partial \\ell_2}{\\partial W_1^1}\\\\\n",
    "& = \\begin{bmatrix} 1 & -1 &0\\\\0 & -1 & 1\\\\ 1 & 1 & -1\\\\1 & 1 & 1\\end{bmatrix} - 0.01\\begin{bmatrix}-1.83 &  1.53 & -3.06\\\\-0.93 &  0.77 & -1.55\\\\0 &  0 & 0\\\\ 0.49 & -0.41 & 0.82\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix} 1.018 & -1.015 &  0.031 \\\\0.009 & -1.008 &  1.016\\\\1.000 &  1.000 & -1.000\\\\0.995 & 1.004 & 0.992 \\end{bmatrix}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "b_2^1 & = b_1^1 - \\eta \\dfrac{\\partial \\ell_2}{\\partial b_1^1}\\\\\n",
    "& = \\begin{bmatrix} 2.2\\\\ 4\\\\ -0.8\\\\ 1.2\\end{bmatrix} - 0.01\\begin{bmatrix}-1.53 \\\\ -0.77 \\\\ 0 \\\\ 0.41 \\end{bmatrix}  \\\\\n",
    "& = \\begin{bmatrix} 0.015\\\\ 1.008\\\\ 1.011\\\\ -1.004\\end{bmatrix}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question a)\n",
      "[[ 2.2]\n",
      " [ 4. ]\n",
      " [-0.8]\n",
      " [ 1.2]]\n",
      "\n",
      "Question b)\n",
      "[[2.2]\n",
      " [4. ]\n",
      " [0. ]\n",
      " [1.2]]\n",
      "\n",
      "Question c)\n",
      "[[7.1]\n",
      " [1.2]\n",
      " [1.1]]\n",
      "\n",
      "Question d)\n",
      "6.0835844697020525\n",
      "\n",
      "Question e)\n",
      "dl/dh2\n",
      "[[-0.92050994 -0.36162891  0.1479391 ]]\n",
      "dl/dW2\n",
      "[[-2.02512188 -3.68203978  0.         -1.10461193]\n",
      " [-0.79558359 -1.44651563  0.         -0.43395469]\n",
      " [ 0.32546602  0.59175639  0.          0.17752692]]\n",
      "dldb2\n",
      "[[-0.92050994]\n",
      " [-0.36162891]\n",
      " [ 0.1479391 ]]\n",
      "\n",
      "Question f)\n",
      "dl/dh1\n",
      "[[-1.52870401 -0.77257085 -1.13419975  0.41094194]]\n",
      "dl/dhbar1\n",
      "[[-1.52870401 -0.77257085  0.          0.41094194]]\n",
      "dl/dW1\n",
      "[[-1.83444482  1.52870401 -3.05740803]\n",
      " [-0.92708502  0.77257085 -1.54514169]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.49313033 -0.41094194  0.82188388]]\n",
      "dldb1\n",
      "[[-1.52870401]\n",
      " [-0.77257085]\n",
      " [-1.13419975]\n",
      " [ 0.41094194]]\n",
      "\n",
      "Question f)\n",
      "W2 at t=2\n",
      "[[ 1.52025122  1.0368204   1.         -0.98895388]\n",
      " [ 0.00795584  0.01446516  1.          1.00433955]\n",
      " [-1.00325466  0.99408244  1.         -1.00177527]]\n",
      "b2 at t=2\n",
      "[[1.0092051 ]\n",
      " [0.00361629]\n",
      " [0.49852061]]\n",
      "W1 at t=2\n",
      "[[ 1.01834445 -1.01528704  0.03057408]\n",
      " [ 0.00927085 -1.00772571  1.01545142]\n",
      " [ 1.          1.         -1.        ]\n",
      " [ 0.9950687   1.00410942  0.99178116]]\n",
      "b1 at t=2\n",
      "[[ 0.01528704]\n",
      " [ 1.00772571]\n",
      " [ 1.011342  ]\n",
      " [-1.00410942]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1.2],\n",
    "              [-1],\n",
    "              [2]])\n",
    "y = np.array([[1.5],\n",
    "              [-1],\n",
    "              [2]])\n",
    "dy =  np.array([-5.6,-2.2,0.9])\n",
    "b1 = np.array([[0],\n",
    "               [1],\n",
    "               [1],\n",
    "               [-1]])\n",
    "W1 = np.array([[1,-1,0],\n",
    "               [0,-1,1],\n",
    "               [1,1,-1],\n",
    "               [1,1,1]])\n",
    "b2 = np.array([[1],\n",
    "               [0],\n",
    "               [0.5]])\n",
    "W2 = np.array([[1.5,1,1,-1],\n",
    "               [0,0,1,1],\n",
    "               [-1,1,1,-1]])\n",
    "h1bar = W1.dot(x) + b1\n",
    "h1 = np.maximum(h1bar,np.zeros(h1bar.shape))\n",
    "h2bar = np.matmul(W2,h1) + b2\n",
    "yhat = h2bar\n",
    "loss = np.sqrt(np.sum((y-yhat)**2))\n",
    "dl2dh2 = np.transpose((1/loss)*(y-yhat))\n",
    "dl2dh1 = dl2dh2.dot(W2)\n",
    "dh1dhbar1 = np.array([[1,  0,  0,  0 ],\n",
    "                      [0,  1,  0,  0 ],\n",
    "                      [0,  0,  0,  0 ],\n",
    "                      [0,  0,  0,  1 ]])\n",
    "dl2dhbar1 = dl2dh1.dot(dh1dhbar1)\n",
    "dl2dW1 = np.transpose(dl2dhbar1).dot(np.transpose(x))\n",
    "dl2dW2 = np.transpose(dl2dh2).dot(np.transpose(h1))\n",
    "W22 = W2-0.01*dl2dW2\n",
    "dl2db2 = np.transpose(dl2dh2)\n",
    "b22 = b2-0.01*dl2db2\n",
    "W21 = W1-0.01*dl2dW1\n",
    "dl2db1 = np.transpose(dl2dh1)\n",
    "b21 = b1-0.01*dl2db1\n",
    "dl2dh1 = dl2dh2.dot(W2)\n",
    "dl2dW2 = np.transpose(dl2dh2).dot(np.transpose(h1))\n",
    "\n",
    "print(\"Question a)\\n{}\".format(h1bar))\n",
    "print(\"\\nQuestion b)\\n{}\".format(h1))\n",
    "print(\"\\nQuestion c)\\n{}\".format(yhat))\n",
    "print(\"\\nQuestion d)\\n{}\".format(loss))\n",
    "print(\"\\nQuestion e)\\ndl/dh2\\n{}\\ndl/dW2\\n{}\\ndldb2\\n{}\".format(dl2dh2,dl2dW2,dl2db2))\n",
    "print(\"\\nQuestion f)\\ndl/dh1\\n{}\\ndl/dhbar1\\n{}\\ndl/dW1\\n{}\\ndldb1\\n{}\".format(dl2dh1,dl2dhbar1,dl2dW1,dl2db1))\n",
    "print(\"\\nQuestion f)\\nW2 at t=2\\n{}\\nb2 at t=2\\n{}\\nW1 at t=2\\n{}\\nb1 at t=2\\n{}\\n\".format(W22,b22,W21,b21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2.5_Py3.6",
   "language": "python",
   "name": "tf25_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
